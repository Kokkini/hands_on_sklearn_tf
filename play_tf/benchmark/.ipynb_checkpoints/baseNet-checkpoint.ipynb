{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbenchmark test for algorithms' efficiency\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "benchmark test for algorithms' efficiency\n",
    "'''\n",
    "# fixed random state\n",
    "# 95% accuracy\n",
    "# test accuracy increase\n",
    "# test training time\n",
    "# no learning rate tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import dropout\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables that doesn't need to be updated every run\n",
    "np_seed = 0\n",
    "tf_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables that need to be changed\n",
    "method = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000,) (10000,)\n",
      "-0.28257035236\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD0CAYAAABjJGgUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyPDCKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275a254bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "np.random.seed(np_seed)\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "train_size = 60000\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data/256-0.5, mnist.target.astype(int), train_size=train_size,shuffle=True)\n",
    "m,n = X_train.shape\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# make sure things go well\n",
    "print(np.mean(X_train[40]))\n",
    "plt.matshow(X_train[40].reshape([28,28]))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(X,y,batch_size):\n",
    "    random_indice = np.random.permutation(X.shape[0])[:batch_size]\n",
    "    return [X[random_indice], y[random_indice]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "test loss : 0.6921 test accuracy : 0.8125\n",
      "train loss: 0.7002 train accuracy: 0.8075\n",
      "elapsed time: 3 lr: 1.0e-01\n",
      "Time to log: 0.6106290817260742\n",
      "Epoch: 10\n",
      "test loss : 0.2432 test accuracy : 0.9302\n",
      "train loss: 0.2254 train accuracy: 0.9339\n",
      "elapsed time: 23 lr: 1.0e-01\n",
      "Time to log: 0.6556308269500732\n",
      "Epoch: 20\n",
      "test loss : 0.1917 test accuracy : 0.9438\n",
      "train loss: 0.1675 train accuracy: 0.9505\n",
      "elapsed time: 45 lr: 1.0e-01\n",
      "Time to log: 0.6958498954772949\n",
      "Epoch: 30\n",
      "test loss : 0.1632 test accuracy : 0.9543\n",
      "train loss: 0.1345 train accuracy: 0.9607\n",
      "elapsed time: 67 lr: 1.0e-01\n",
      "Time to log: 0.8171780109405518\n",
      "Epoch: 40\n",
      "test loss : 0.1443 test accuracy : 0.9566\n",
      "train loss: 0.1123 train accuracy: 0.9664\n",
      "elapsed time: 88 lr: 1.0e-01\n",
      "Time to log: 0.567225456237793\n",
      "Epoch: 50\n",
      "test loss : 0.1340 test accuracy : 0.9610\n",
      "train loss: 0.0972 train accuracy: 0.9703\n",
      "elapsed time: 109 lr: 1.0e-01\n",
      "Time to log: 0.6691560745239258\n",
      "Epoch: 60\n",
      "test loss : 0.1246 test accuracy : 0.9647\n",
      "train loss: 0.0823 train accuracy: 0.9760\n",
      "elapsed time: 130 lr: 1.0e-01\n",
      "Time to log: 0.5619659423828125\n",
      "Epoch: 70\n",
      "test loss : 0.1184 test accuracy : 0.9644\n",
      "train loss: 0.0737 train accuracy: 0.9783\n",
      "elapsed time: 151 lr: 1.0e-01\n",
      "Time to log: 0.5577366352081299\n",
      "Epoch: 80\n",
      "test loss : 0.1228 test accuracy : 0.9646\n",
      "train loss: 0.0767 train accuracy: 0.9771\n",
      "elapsed time: 170 lr: 1.0e-01\n",
      "Time to log: 0.5636062622070312\n",
      "Epoch: 90\n",
      "test loss : 0.1131 test accuracy : 0.9664\n",
      "train loss: 0.0624 train accuracy: 0.9813\n",
      "elapsed time: 190 lr: 1.0e-01\n",
      "Time to log: 0.5986323356628418\n",
      "Epoch: 100\n",
      "test loss : 0.1085 test accuracy : 0.9691\n",
      "train loss: 0.0554 train accuracy: 0.9838\n",
      "elapsed time: 211 lr: 1.0e-01\n",
      "Time to log: 0.6418859958648682\n",
      "Epoch: 110\n",
      "test loss : 0.1162 test accuracy : 0.9680\n",
      "train loss: 0.0538 train accuracy: 0.9835\n",
      "elapsed time: 232 lr: 1.0e-01\n",
      "Time to log: 0.654242992401123\n",
      "Epoch: 120\n",
      "test loss : 0.1060 test accuracy : 0.9695\n",
      "train loss: 0.0422 train accuracy: 0.9882\n",
      "elapsed time: 253 lr: 1.0e-01\n",
      "Time to log: 0.6289119720458984\n",
      "Epoch: 130\n",
      "test loss : 0.1120 test accuracy : 0.9694\n",
      "train loss: 0.0427 train accuracy: 0.9876\n",
      "elapsed time: 274 lr: 1.0e-01\n",
      "Time to log: 0.5815720558166504\n",
      "Epoch: 140\n",
      "test loss : 0.1058 test accuracy : 0.9701\n",
      "train loss: 0.0349 train accuracy: 0.9905\n",
      "elapsed time: 296 lr: 1.0e-01\n",
      "Time to log: 0.6788063049316406\n",
      "Epoch: 150\n",
      "test loss : 0.1060 test accuracy : 0.9716\n",
      "train loss: 0.0307 train accuracy: 0.9919\n",
      "elapsed time: 318 lr: 1.0e-01\n",
      "Time to log: 0.6572568416595459\n",
      "Epoch: 160\n",
      "test loss : 0.1089 test accuracy : 0.9704\n",
      "train loss: 0.0304 train accuracy: 0.9917\n",
      "elapsed time: 339 lr: 1.0e-01\n",
      "Time to log: 0.5845701694488525\n",
      "Epoch: 170\n",
      "test loss : 0.1064 test accuracy : 0.9720\n",
      "train loss: 0.0251 train accuracy: 0.9938\n",
      "elapsed time: 359 lr: 1.0e-01\n",
      "Time to log: 0.6236672401428223\n",
      "Epoch: 180\n",
      "test loss : 0.1082 test accuracy : 0.9704\n",
      "train loss: 0.0232 train accuracy: 0.9944\n",
      "elapsed time: 380 lr: 1.0e-01\n",
      "Time to log: 0.5714859962463379\n",
      "Epoch: 190\n",
      "test loss : 0.1099 test accuracy : 0.9718\n",
      "train loss: 0.0213 train accuracy: 0.9951\n",
      "elapsed time: 401 lr: 1.0e-01\n",
      "Time to log: 0.5950977802276611\n",
      "Epoch: 200\n",
      "test loss : 0.1083 test accuracy : 0.9721\n",
      "train loss: 0.0185 train accuracy: 0.9963\n",
      "elapsed time: 421 lr: 1.0e-01\n",
      "Time to log: 0.5620112419128418\n",
      "Epoch: 210\n",
      "test loss : 0.1118 test accuracy : 0.9701\n",
      "train loss: 0.0178 train accuracy: 0.9963\n",
      "elapsed time: 444 lr: 1.0e-01\n",
      "Time to log: 0.6823136806488037\n",
      "Epoch: 220\n",
      "test loss : 0.1414 test accuracy : 0.9600\n",
      "train loss: 0.0884 train accuracy: 0.9723\n",
      "elapsed time: 466 lr: 1.0e-01\n",
      "Time to log: 0.5915737152099609\n",
      "Epoch: 230\n",
      "test loss : 0.1236 test accuracy : 0.9654\n",
      "train loss: 0.0665 train accuracy: 0.9797\n",
      "elapsed time: 488 lr: 1.0e-01\n",
      "Time to log: 0.6953659057617188\n",
      "Test accuracy : 0.9648\n",
      "Train accuracy: 0.980616666667\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"logs\"\n",
    "logdir = \"{}/base-{}\".format(root_logdir, now)\n",
    "root_savedir = \"checkpoints\"\n",
    "savedir = \"{}/base-{}\".format(root_savedir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(tf_seed) # set random seed\n",
    "n_neurons = 50\n",
    "lr_start = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=[None, 784])\n",
    "y = tf.placeholder(tf.int64, shape=[None])\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "keep_prob = 0.75\n",
    "\n",
    "with tf.variable_scope(\"forward\"):\n",
    "    dense_1 = fully_connected(X, 64, scope=\"dense_1\")\n",
    "    dense_2 = fully_connected(dense_1, 32, scope=\"dense_2\")\n",
    "    output = fully_connected(dense_2, 10, activation_fn=None, scope=\"output\")\n",
    "    \n",
    "with tf.variable_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y,name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy,name=\"loss\")\n",
    "    \n",
    "with tf.variable_scope(\"annealing\"):\n",
    "    lr = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
    "    \n",
    "with tf.variable_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.variable_scope(\"eval\"):\n",
    "    pred = tf.argmax(output,axis=1,name=\"prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y),tf.float64))\n",
    "with tf.variable_scope(\"save\"):\n",
    "    saver = tf.train.Saver()\n",
    "    train_loss_summary = tf.summary.scalar(\"train_loss\", loss)\n",
    "    test_loss_summary = tf.summary.scalar(\"test_loss\", loss)\n",
    "    train_accuracy_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "    test_accuracy_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "    filewriter = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    \n",
    "# execution phase\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(np_seed) # set random seed\n",
    "    start_running = time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_epochs = 200\n",
    "    batch_size = 1000\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        lr_ = lr_start\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch,lr:lr_,is_training:True})\n",
    "        if epoch%10==0:\n",
    "            start_time = time()\n",
    "            test_loss_summary_,  test_accuracy_summary_,  loss_test,  accuracy_test  = sess.run([test_loss_summary,\n",
    "                                                                                                 test_accuracy_summary,\n",
    "                                                                                                 loss, \n",
    "                                                                                                 accuracy],\n",
    "                                                                                                feed_dict={X:X_test,y:y_test,is_training:False})\n",
    "            train_loss_summary_, train_accuracy_summary_, loss_train, accuracy_train = sess.run([train_loss_summary,\n",
    "                                                                                                 train_accuracy_summary,\n",
    "                                                                                                 loss,\n",
    "                                                                                                 accuracy], \n",
    "                                                                                                feed_dict={X:X_train,y:y_train,is_training:False})\n",
    "            \n",
    "            filewriter.add_summary(test_loss_summary_,epoch)\n",
    "            filewriter.add_summary(train_loss_summary_,epoch)\n",
    "            filewriter.add_summary(test_accuracy_summary_,epoch)\n",
    "            filewriter.add_summary(train_accuracy_summary_,epoch)\n",
    "            \n",
    "            print(\"Epoch:\",epoch)\n",
    "            print(\"test loss : %.4f\" % loss_test , \"test accuracy : %.4f\" % accuracy_test)\n",
    "            print(\"train loss: %.4f\" % loss_train, \"train accuracy: %.4f\" % accuracy_train)\n",
    "            print(\"elapsed time: %.0f\" % (time()-start_running), \"lr: %.1e\" % lr_)\n",
    "            print(\"Time to log:\",time()-start_time)\n",
    "        if epoch%50==0:\n",
    "            saver.save(sess,savedir+\"/model.ckpt\")\n",
    "    saver.save(sess,savedir+\"/model_final.ckpt\")\n",
    "    train_accuracy = np.mean(sess.run(pred, feed_dict={X:X_train, y:y_train,is_training:False})==y_train)\n",
    "    test_accuracy = np.mean(sess.run(pred, feed_dict={X:X_test, y:y_test,is_training:False})==y_test)\n",
    "    print(\"Test accuracy :\", test_accuracy)\n",
    "    print(\"Train accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
