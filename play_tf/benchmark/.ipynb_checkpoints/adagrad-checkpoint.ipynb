{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbenchmark test for algorithms' efficiency\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "benchmark test for algorithms' efficiency\n",
    "'''\n",
    "# fixed random state\n",
    "# 95% accuracy\n",
    "# test accuracy increase\n",
    "# test training time\n",
    "# no learning rate tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import dropout\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables that doesn't need to be updated every run\n",
    "np_seed = 4\n",
    "tf_seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables that need to be changed\n",
    "method = \"adagrad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000,) (10000,)\n",
      "-0.470742984694\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD0CAYAAABjJGgUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqQSKiCiVQBERpRIoIqLU/wPjyi3nYqyVbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa8fb89470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "np.random.seed(np_seed)\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "train_size = 60000\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data/256-0.5, mnist.target.astype(int), train_size=train_size,shuffle=True)\n",
    "m,n = X_train.shape\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# make sure things go well\n",
    "print(np.mean(X_train[40]))\n",
    "plt.matshow(X_train[40].reshape([28,28]))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(X,y,batch_size):\n",
    "    random_indice = np.random.permutation(X.shape[0])[:batch_size]\n",
    "    return [X[random_indice], y[random_indice]]\n",
    "def get_lr(lr_start,epoch):\n",
    "    return lr_start/(2**(epoch/300.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "test loss : 1.3343 test accuracy : 0.5161\n",
      "train loss: 1.3313 train accuracy: 0.5147\n",
      "elapsed time: 4 lr: 1.0e-01\n",
      "Time to log: 0.7905433177947998\n",
      "Epoch: 10\n",
      "test loss : 0.1831 test accuracy : 0.9439\n",
      "train loss: 0.1733 train accuracy: 0.9477\n",
      "elapsed time: 31 lr: 1.0e-01\n",
      "Time to log: 0.7107348442077637\n",
      "Epoch: 20\n",
      "test loss : 0.1425 test accuracy : 0.9562\n",
      "train loss: 0.1190 train accuracy: 0.9645\n",
      "elapsed time: 58 lr: 1.0e-01\n",
      "Time to log: 0.7515242099761963\n",
      "Epoch: 30\n",
      "test loss : 0.1500 test accuracy : 0.9539\n",
      "train loss: 0.1249 train accuracy: 0.9620\n",
      "elapsed time: 85 lr: 1.0e-01\n",
      "Time to log: 0.7827465534210205\n",
      "Epoch: 40\n",
      "test loss : 0.1354 test accuracy : 0.9579\n",
      "train loss: 0.0999 train accuracy: 0.9697\n",
      "elapsed time: 111 lr: 1.0e-01\n",
      "Time to log: 0.7870874404907227\n",
      "Epoch: 50\n",
      "test loss : 0.1261 test accuracy : 0.9613\n",
      "train loss: 0.0831 train accuracy: 0.9754\n",
      "elapsed time: 138 lr: 1.0e-01\n",
      "Time to log: 0.8184044361114502\n",
      "Epoch: 60\n",
      "test loss : 0.1238 test accuracy : 0.9635\n",
      "train loss: 0.0736 train accuracy: 0.9776\n",
      "elapsed time: 165 lr: 1.0e-01\n",
      "Time to log: 0.7305374145507812\n",
      "Epoch: 70\n",
      "test loss : 0.1319 test accuracy : 0.9590\n",
      "train loss: 0.0751 train accuracy: 0.9766\n",
      "elapsed time: 191 lr: 1.0e-01\n",
      "Time to log: 0.7867445945739746\n",
      "Epoch: 80\n",
      "test loss : 0.1216 test accuracy : 0.9636\n",
      "train loss: 0.0600 train accuracy: 0.9818\n",
      "elapsed time: 218 lr: 1.0e-01\n",
      "Time to log: 0.9102261066436768\n",
      "Epoch: 90\n",
      "test loss : 0.1214 test accuracy : 0.9635\n",
      "train loss: 0.0548 train accuracy: 0.9835\n",
      "elapsed time: 244 lr: 1.0e-01\n",
      "Time to log: 0.8203873634338379\n",
      "Epoch: 100\n",
      "test loss : 0.1229 test accuracy : 0.9623\n",
      "train loss: 0.0496 train accuracy: 0.9855\n",
      "elapsed time: 272 lr: 1.0e-01\n",
      "Time to log: 0.8206984996795654\n",
      "Epoch: 110\n",
      "test loss : 0.1232 test accuracy : 0.9651\n",
      "train loss: 0.0463 train accuracy: 0.9866\n",
      "elapsed time: 298 lr: 1.0e-01\n",
      "Time to log: 0.7252721786499023\n",
      "Epoch: 120\n",
      "test loss : 0.1232 test accuracy : 0.9636\n",
      "train loss: 0.0397 train accuracy: 0.9886\n",
      "elapsed time: 325 lr: 1.0e-01\n",
      "Time to log: 0.7308533191680908\n",
      "Epoch: 130\n",
      "test loss : 0.1241 test accuracy : 0.9646\n",
      "train loss: 0.0376 train accuracy: 0.9893\n",
      "elapsed time: 352 lr: 1.0e-01\n",
      "Time to log: 0.8690931797027588\n",
      "Epoch: 140\n",
      "test loss : 0.1364 test accuracy : 0.9625\n",
      "train loss: 0.0429 train accuracy: 0.9865\n",
      "elapsed time: 380 lr: 1.0e-01\n",
      "Time to log: 0.8039391040802002\n",
      "Epoch: 150\n",
      "test loss : 0.3990 test accuracy : 0.8848\n",
      "train loss: 0.3494 train accuracy: 0.8952\n",
      "elapsed time: 408 lr: 1.0e-01\n",
      "Time to log: 0.7833507061004639\n",
      "Epoch: 160\n",
      "test loss : 0.1506 test accuracy : 0.9553\n",
      "train loss: 0.0990 train accuracy: 0.9686\n",
      "elapsed time: 436 lr: 1.0e-01\n",
      "Time to log: 0.7654879093170166\n",
      "Epoch: 170\n",
      "test loss : 0.1433 test accuracy : 0.9562\n",
      "train loss: 0.0837 train accuracy: 0.9724\n",
      "elapsed time: 462 lr: 1.0e-01\n",
      "Time to log: 0.7156839370727539\n",
      "Epoch: 180\n",
      "test loss : 0.1385 test accuracy : 0.9580\n",
      "train loss: 0.0764 train accuracy: 0.9756\n",
      "elapsed time: 489 lr: 1.0e-01\n",
      "Time to log: 0.7513761520385742\n",
      "Epoch: 190\n",
      "test loss : 0.1356 test accuracy : 0.9607\n",
      "train loss: 0.0649 train accuracy: 0.9800\n",
      "elapsed time: 516 lr: 1.0e-01\n",
      "Time to log: 0.7564599514007568\n",
      "Epoch: 200\n",
      "test loss : 0.1323 test accuracy : 0.9625\n",
      "train loss: 0.0583 train accuracy: 0.9824\n",
      "elapsed time: 542 lr: 1.0e-01\n",
      "Time to log: 0.7297654151916504\n",
      "Epoch: 210\n",
      "test loss : 0.1437 test accuracy : 0.9589\n",
      "train loss: 0.0625 train accuracy: 0.9796\n",
      "elapsed time: 569 lr: 1.0e-01\n",
      "Time to log: 0.7138981819152832\n",
      "Epoch: 220\n",
      "test loss : 0.1393 test accuracy : 0.9607\n",
      "train loss: 0.0548 train accuracy: 0.9831\n",
      "elapsed time: 596 lr: 1.0e-01\n",
      "Time to log: 0.7779228687286377\n",
      "Epoch: 230\n",
      "test loss : 0.1336 test accuracy : 0.9635\n",
      "train loss: 0.0480 train accuracy: 0.9860\n",
      "elapsed time: 623 lr: 1.0e-01\n",
      "Time to log: 0.7654745578765869\n",
      "Epoch: 240\n",
      "test loss : 0.1347 test accuracy : 0.9631\n",
      "train loss: 0.0477 train accuracy: 0.9855\n",
      "elapsed time: 650 lr: 1.0e-01\n",
      "Time to log: 0.7479193210601807\n",
      "Epoch: 250\n",
      "test loss : 0.1369 test accuracy : 0.9618\n",
      "train loss: 0.0455 train accuracy: 0.9863\n",
      "elapsed time: 676 lr: 1.0e-01\n",
      "Time to log: 0.7754027843475342\n",
      "Epoch: 260\n",
      "test loss : 0.1433 test accuracy : 0.9611\n",
      "train loss: 0.0444 train accuracy: 0.9865\n",
      "elapsed time: 704 lr: 1.0e-01\n",
      "Time to log: 0.7901654243469238\n",
      "Epoch: 270\n",
      "test loss : 0.1412 test accuracy : 0.9613\n",
      "train loss: 0.0429 train accuracy: 0.9875\n",
      "elapsed time: 731 lr: 1.0e-01\n",
      "Time to log: 0.7571506500244141\n",
      "Epoch: 280\n",
      "test loss : 0.1466 test accuracy : 0.9617\n",
      "train loss: 0.0395 train accuracy: 0.9885\n",
      "elapsed time: 758 lr: 1.0e-01\n",
      "Time to log: 0.7552237510681152\n",
      "Epoch: 290\n",
      "test loss : 0.1608 test accuracy : 0.9578\n",
      "train loss: 0.0482 train accuracy: 0.9847\n",
      "elapsed time: 785 lr: 1.0e-01\n",
      "Time to log: 0.7920145988464355\n",
      "Test accuracy : 0.9627\n",
      "Train accuracy: 0.9894\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "now = datetime.utcnow().strftime(\"%y%m%d-%H%M\")\n",
    "root_logdir = \"logs\"\n",
    "logdir_train = \"{}/{}_{}_train\".format(root_logdir, method, now)\n",
    "logdir_test = \"{}/{}_{}_test\".format(root_logdir,method,now)\n",
    "root_savedir = \"checkpoints\"\n",
    "savedir = \"{}/base-{}\".format(root_savedir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(tf_seed) # set random seed\n",
    "n_neurons = 50\n",
    "lr_start = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=[None, 784])\n",
    "y = tf.placeholder(tf.int64, shape=[None])\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "keep_prob = 0.75\n",
    "\n",
    "with tf.variable_scope(\"forward\"):\n",
    "    dense_1 = fully_connected(X, 64, scope=\"dense_1\")\n",
    "    dense_2 = fully_connected(dense_1, 32, scope=\"dense_2\")\n",
    "    output = fully_connected(dense_2, 10, activation_fn=None, scope=\"output\")\n",
    "    \n",
    "with tf.variable_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y,name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy,name=\"loss\")\n",
    "    \n",
    "with tf.variable_scope(\"annealing\"):\n",
    "    lr = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
    "    \n",
    "with tf.variable_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(lr)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.variable_scope(\"eval\"):\n",
    "    pred = tf.argmax(output,axis=1,name=\"prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y),tf.float64))\n",
    "with tf.variable_scope(\"save\"):\n",
    "    saver = tf.train.Saver()\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary = tf.summary.merge_all()\n",
    "    writer_test = tf.summary.FileWriter(logdir_test,tf.get_default_graph())\n",
    "    writer_train = tf.summary.FileWriter(logdir_train,tf.get_default_graph())\n",
    "# execution phase\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(np_seed) # set random seed\n",
    "    start_running = time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_epochs = 300\n",
    "    batch_size = 1000\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        lr_ = get_lr(lr_start, epoch)\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch,lr:lr_,is_training:True})\n",
    "        if epoch%10==0:\n",
    "            start_time = time()\n",
    "            summary_test, loss_test,  accuracy_test  = sess.run([summary,loss,accuracy],feed_dict={X:X_test,y:y_test,is_training:False})\n",
    "            writer_test.add_summary(summary_test,epoch)\n",
    "            summary_train,loss_train,  accuracy_train  = sess.run([summary,loss,accuracy],feed_dict={X:X_train,y:y_train,is_training:False})\n",
    "            writer_train.add_summary(summary_train,epoch)\n",
    "            \n",
    "            print(\"Epoch:\",epoch)\n",
    "            print(\"test loss : %.4f\" % loss_test , \"test accuracy : %.4f\" % accuracy_test)\n",
    "            print(\"train loss: %.4f\" % loss_train, \"train accuracy: %.4f\" % accuracy_train)\n",
    "            print(\"elapsed time: %.0f\" % (time()-start_running), \"lr: %.1e\" % lr_)\n",
    "            print(\"Time to log:\",time()-start_time)\n",
    "        if epoch%50==0:\n",
    "            saver.save(sess,savedir+\"/model.ckpt\")\n",
    "    saver.save(sess,savedir+\"/model_final.ckpt\")\n",
    "    train_accuracy = np.mean(sess.run(pred, feed_dict={X:X_train, y:y_train,is_training:False})==y_train)\n",
    "    test_accuracy = np.mean(sess.run(pred, feed_dict={X:X_test, y:y_test,is_training:False})==y_test)\n",
    "    print(\"Test accuracy :\", test_accuracy)\n",
    "    print(\"Train accuracy:\", train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
