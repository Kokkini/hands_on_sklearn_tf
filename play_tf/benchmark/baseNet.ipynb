{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbenchmark test for algorithms' efficiency\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "benchmark test for algorithms' efficiency\n",
    "'''\n",
    "# fixed random state\n",
    "# 95% accuracy\n",
    "# test accuracy increase\n",
    "# test training time\n",
    "# no learning rate tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import dropout\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables that doesn't need to be updated every run\n",
    "np_seed = 4\n",
    "tf_seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables that need to be changed\n",
    "method = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000,) (10000,)\n",
      "-0.470742984694\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD0CAYAAABjJGgUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqQSKiCiVQBERpRIoIqLU/wPjyi3nYqyVbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x167808fa278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "np.random.seed(np_seed)\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "train_size = 60000\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data/256-0.5, mnist.target.astype(int), train_size=train_size,shuffle=True)\n",
    "m,n = X_train.shape\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# make sure things go well\n",
    "print(np.mean(X_train[40]))\n",
    "plt.matshow(X_train[40].reshape([28,28]))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(X,y,batch_size):\n",
    "    random_indice = np.random.permutation(X.shape[0])[:batch_size]\n",
    "    return [X[random_indice], y[random_indice]]\n",
    "def get_lr(lr_start,epoch):\n",
    "    return lr_start/(2**(epoch/300.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "test loss : 0.6985 test accuracy : 0.7625\n",
      "train loss: 0.6923 train accuracy: 0.7646\n",
      "elapsed time: 3 lr: 1.0e-01\n",
      "Time to log: 0.8108322620391846\n",
      "Epoch: 10\n",
      "test loss : 0.2319 test accuracy : 0.9300\n",
      "train loss: 0.2275 train accuracy: 0.9336\n",
      "elapsed time: 26 lr: 9.8e-02\n",
      "Time to log: 0.5655639171600342\n",
      "Epoch: 20\n",
      "test loss : 0.1857 test accuracy : 0.9419\n",
      "train loss: 0.1753 train accuracy: 0.9478\n",
      "elapsed time: 45 lr: 9.5e-02\n",
      "Time to log: 0.6980974674224854\n",
      "Epoch: 30\n",
      "test loss : 0.1480 test accuracy : 0.9556\n",
      "train loss: 0.1301 train accuracy: 0.9632\n",
      "elapsed time: 66 lr: 9.3e-02\n",
      "Time to log: 0.573045015335083\n",
      "Epoch: 40\n",
      "test loss : 0.1326 test accuracy : 0.9595\n",
      "train loss: 0.1076 train accuracy: 0.9690\n",
      "elapsed time: 86 lr: 9.1e-02\n",
      "Time to log: 0.5670168399810791\n",
      "Epoch: 50\n",
      "test loss : 0.1206 test accuracy : 0.9624\n",
      "train loss: 0.0913 train accuracy: 0.9736\n",
      "elapsed time: 106 lr: 8.9e-02\n",
      "Time to log: 0.5566930770874023\n",
      "Epoch: 60\n",
      "test loss : 0.1167 test accuracy : 0.9648\n",
      "train loss: 0.0797 train accuracy: 0.9769\n",
      "elapsed time: 126 lr: 8.7e-02\n",
      "Time to log: 0.5899059772491455\n",
      "Epoch: 70\n",
      "test loss : 0.1237 test accuracy : 0.9624\n",
      "train loss: 0.0823 train accuracy: 0.9748\n",
      "elapsed time: 146 lr: 8.5e-02\n",
      "Time to log: 0.57354736328125\n",
      "Epoch: 80\n",
      "test loss : 0.1067 test accuracy : 0.9670\n",
      "train loss: 0.0612 train accuracy: 0.9825\n",
      "elapsed time: 165 lr: 8.3e-02\n",
      "Time to log: 0.5811069011688232\n",
      "Epoch: 90\n",
      "test loss : 0.1048 test accuracy : 0.9681\n",
      "train loss: 0.0559 train accuracy: 0.9840\n",
      "elapsed time: 186 lr: 8.1e-02\n",
      "Time to log: 0.7449805736541748\n",
      "Epoch: 100\n",
      "test loss : 0.1031 test accuracy : 0.9685\n",
      "train loss: 0.0489 train accuracy: 0.9865\n",
      "elapsed time: 208 lr: 7.9e-02\n",
      "Time to log: 0.9044220447540283\n",
      "Epoch: 110\n",
      "test loss : 0.1021 test accuracy : 0.9684\n",
      "train loss: 0.0450 train accuracy: 0.9877\n",
      "elapsed time: 232 lr: 7.8e-02\n",
      "Time to log: 0.6888730525970459\n",
      "Epoch: 120\n",
      "test loss : 0.1019 test accuracy : 0.9681\n",
      "train loss: 0.0401 train accuracy: 0.9895\n",
      "elapsed time: 252 lr: 7.6e-02\n",
      "Time to log: 0.5805606842041016\n",
      "Epoch: 130\n",
      "test loss : 0.1023 test accuracy : 0.9685\n",
      "train loss: 0.0368 train accuracy: 0.9903\n",
      "elapsed time: 274 lr: 7.4e-02\n",
      "Time to log: 0.589543342590332\n",
      "Epoch: 140\n",
      "test loss : 0.1041 test accuracy : 0.9685\n",
      "train loss: 0.0351 train accuracy: 0.9907\n",
      "elapsed time: 295 lr: 7.2e-02\n",
      "Time to log: 0.7936108112335205\n",
      "Epoch: 150\n",
      "test loss : 0.1047 test accuracy : 0.9694\n",
      "train loss: 0.0331 train accuracy: 0.9912\n",
      "elapsed time: 321 lr: 7.1e-02\n",
      "Time to log: 0.6788043975830078\n",
      "Epoch: 160\n",
      "test loss : 0.1046 test accuracy : 0.9691\n",
      "train loss: 0.0298 train accuracy: 0.9929\n",
      "elapsed time: 346 lr: 6.9e-02\n",
      "Time to log: 0.7384638786315918\n",
      "Epoch: 170\n",
      "test loss : 0.1043 test accuracy : 0.9685\n",
      "train loss: 0.0268 train accuracy: 0.9938\n",
      "elapsed time: 370 lr: 6.8e-02\n",
      "Time to log: 0.6758072376251221\n",
      "Epoch: 180\n",
      "test loss : 0.1054 test accuracy : 0.9697\n",
      "train loss: 0.0251 train accuracy: 0.9946\n",
      "elapsed time: 395 lr: 6.6e-02\n",
      "Time to log: 0.6261658668518066\n",
      "Epoch: 190\n",
      "test loss : 0.1070 test accuracy : 0.9702\n",
      "train loss: 0.0232 train accuracy: 0.9950\n",
      "elapsed time: 419 lr: 6.4e-02\n",
      "Time to log: 0.7745616436004639\n",
      "Epoch: 200\n",
      "test loss : 0.1063 test accuracy : 0.9693\n",
      "train loss: 0.0211 train accuracy: 0.9959\n",
      "elapsed time: 443 lr: 6.3e-02\n",
      "Time to log: 0.8111572265625\n",
      "Epoch: 210\n",
      "test loss : 0.1078 test accuracy : 0.9689\n",
      "train loss: 0.0200 train accuracy: 0.9964\n",
      "elapsed time: 467 lr: 6.2e-02\n",
      "Time to log: 0.7294406890869141\n",
      "Epoch: 220\n",
      "test loss : 0.1085 test accuracy : 0.9699\n",
      "train loss: 0.0185 train accuracy: 0.9967\n",
      "elapsed time: 492 lr: 6.0e-02\n",
      "Time to log: 0.701866865158081\n",
      "Epoch: 230\n",
      "test loss : 0.1091 test accuracy : 0.9701\n",
      "train loss: 0.0174 train accuracy: 0.9970\n",
      "elapsed time: 515 lr: 5.9e-02\n",
      "Time to log: 0.7194108963012695\n",
      "Epoch: 240\n",
      "test loss : 0.1109 test accuracy : 0.9693\n",
      "train loss: 0.0170 train accuracy: 0.9969\n",
      "elapsed time: 541 lr: 5.7e-02\n",
      "Time to log: 1.0394814014434814\n",
      "Epoch: 250\n",
      "test loss : 0.1113 test accuracy : 0.9690\n",
      "train loss: 0.0158 train accuracy: 0.9973\n",
      "elapsed time: 567 lr: 5.6e-02\n",
      "Time to log: 1.030099868774414\n",
      "Epoch: 260\n",
      "test loss : 0.1130 test accuracy : 0.9701\n",
      "train loss: 0.0147 train accuracy: 0.9977\n",
      "elapsed time: 592 lr: 5.5e-02\n",
      "Time to log: 0.6938445568084717\n",
      "Epoch: 270\n",
      "test loss : 0.1137 test accuracy : 0.9701\n",
      "train loss: 0.0143 train accuracy: 0.9980\n",
      "elapsed time: 617 lr: 5.4e-02\n",
      "Time to log: 0.7179090976715088\n",
      "Epoch: 280\n",
      "test loss : 0.1163 test accuracy : 0.9696\n",
      "train loss: 0.0136 train accuracy: 0.9982\n",
      "elapsed time: 642 lr: 5.2e-02\n",
      "Time to log: 0.7996230125427246\n",
      "Epoch: 290\n",
      "test loss : 0.1159 test accuracy : 0.9701\n",
      "train loss: 0.0126 train accuracy: 0.9985\n",
      "elapsed time: 668 lr: 5.1e-02\n",
      "Time to log: 0.7284340858459473\n",
      "Test accuracy : 0.969\n",
      "Train accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "# construction phase\n",
    "now = datetime.utcnow().strftime(\"%y%m%d-%H%M\")\n",
    "root_logdir = \"logs\"\n",
    "logdir_train = \"{}/{}_{}_train\".format(root_logdir, method, now)\n",
    "logdir_test = \"{}/{}_{}_test\".format(root_logdir,method,now)\n",
    "root_savedir = \"checkpoints\"\n",
    "savedir = \"{}/base-{}\".format(root_savedir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(tf_seed) # set random seed\n",
    "n_neurons = 50\n",
    "lr_start = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=[None, 784])\n",
    "y = tf.placeholder(tf.int64, shape=[None])\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "keep_prob = 0.75\n",
    "\n",
    "with tf.variable_scope(\"forward\"):\n",
    "    dense_1 = fully_connected(X, 64, scope=\"dense_1\")\n",
    "    dense_2 = fully_connected(dense_1, 32, scope=\"dense_2\")\n",
    "    output = fully_connected(dense_2, 10, activation_fn=None, scope=\"output\")\n",
    "    \n",
    "with tf.variable_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y,name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy,name=\"loss\")\n",
    "    \n",
    "with tf.variable_scope(\"annealing\"):\n",
    "    lr = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
    "    \n",
    "with tf.variable_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.variable_scope(\"eval\"):\n",
    "    pred = tf.argmax(output,axis=1,name=\"prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y),tf.float64))\n",
    "with tf.variable_scope(\"save\"):\n",
    "    saver = tf.train.Saver()\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary = tf.summary.merge_all()\n",
    "    writer_test = tf.summary.FileWriter(logdir_test,tf.get_default_graph())\n",
    "    writer_train = tf.summary.FileWriter(logdir_train,tf.get_default_graph())\n",
    "# execution phase\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(np_seed) # set random seed\n",
    "    start_running = time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_epochs = 300\n",
    "    batch_size = 1000\n",
    "    n_batches = int(np.ceil(m/batch_size))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        lr_ = get_lr(lr_start,epoch)\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch,lr:lr_,is_training:True})\n",
    "        if epoch%10==0:\n",
    "            start_time = time()\n",
    "            summary_test, loss_test,  accuracy_test  = sess.run([summary,loss,accuracy],feed_dict={X:X_test,y:y_test,is_training:False})\n",
    "            writer_test.add_summary(summary_test,epoch)\n",
    "            summary_train,loss_train,  accuracy_train  = sess.run([summary,loss,accuracy],feed_dict={X:X_train,y:y_train,is_training:False})\n",
    "            writer_train.add_summary(summary_train,epoch)\n",
    "            \n",
    "            print(\"Epoch:\",epoch)\n",
    "            print(\"test loss : %.4f\" % loss_test , \"test accuracy : %.4f\" % accuracy_test)\n",
    "            print(\"train loss: %.4f\" % loss_train, \"train accuracy: %.4f\" % accuracy_train)\n",
    "            print(\"elapsed time: %.0f\" % (time()-start_running), \"lr: %.1e\" % lr_)\n",
    "            print(\"Time to log:\",time()-start_time)\n",
    "        if epoch%50==0:\n",
    "            saver.save(sess,savedir+\"/model.ckpt\")\n",
    "    saver.save(sess,savedir+\"/model_final.ckpt\")\n",
    "    train_accuracy = np.mean(sess.run(pred, feed_dict={X:X_train, y:y_train,is_training:False})==y_train)\n",
    "    test_accuracy = np.mean(sess.run(pred, feed_dict={X:X_test, y:y_test,is_training:False})==y_test)\n",
    "    print(\"Test accuracy :\", test_accuracy)\n",
    "    print(\"Train accuracy:\", train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
